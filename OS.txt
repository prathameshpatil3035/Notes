Basic Functionalities of OS:
    1. Resource Management
    2. Process Management (CPU Scheduling)
    3. Storage Management (HD - File system)
    4. Memory Management (RAM).

OS  (user -> OS -> HardWare)

    two modes of operations in which a program can execute : 
    1. user mode 
        If a program is executing in user mode, it does not have direct access to system resources such as memory, hardware, or privileged instructions.
        Instead, the program must request access to these resources through system calls that the operating system manages.
        A crash in a program executing in user mode does not affect the entire system to crash, as the program operates within its isolated environment. The crash is restricted to that program and does not compromise the operating system's stability.   

    2. kernal mode (privileged mode)
        If a program is executing in kernel mode, it has direct access to system resources such as memory, hardware, and privileged CPU instructions.
        Programs in this mode can execute critical tasks, like managing device drivers, performing I/O operations, or controlling system memory.
        If a program executing in kernel mode crashes, it can cause the entire system to crash because the program operates with unrestricted access to all system resources. This makes stability in kernel mode operations critical.

    In an operating system, transitioning from user mode to kernel mode occurs when a process needs to access privileged resources such as hardware (e.g., memory, monitor, hard disk) that are protected from direct user access. This transition typically happens through a system call.
    The process of switching between user mode and kernel mode is called a context switch or mode switch

    System call :
        System call provide an interface(between user mode to kernal mode) to the service made available by an operating system. System call is a way of communication through which user mode can interact with kernal mode.
        System call is a programmatic way in which a computer program requests a service  from the kernal of the operating system, these calls are generally available as routines written in C and C++
    
    Structure of Operating System:
        MSDOS structure (Microsoft disk operating system):
            msdos written on the intel 8088 and this intel 8088 does not provide dual mode or any hardware protection.
            It looks like layered structure but all the layers has direct access to hardware. so if program using hardware fails then the entire system will crash. 

            +----------------------------------------------------------+
            | Application programs                                     |
            +----------------------------------------------------------+
                                                                    |
            +---------------------------------------------------+   |
            | Resident system programs                          |   |
            +---------------------------------------------------+   |
                                                            |       |
            +-------------------------------------------+   |       |
            | Device drivers                            |   |       |
            +-------------------------------------------+   |       |
                                                |           |       |
                                                v           v       v
            +----------------------------------------------------------+
            | ROM BIOS device drivers (Hardware)                       |
            +----------------------------------------------------------+

        Monolithic Structure:
            this structure was followed by earlier unix operating system.
            the problem with this is that there are too many functions packed into one level (kernal level) and for this reason it is called as monolithic structure and this makes its implementation and maintenance very difficult. 

            +--------------------------------------------------------------------+
            |                           (The Users)                              |
            +--------------------------------------------------------------------+
            |                       Shell and Commands                           |
            |                   Compillers and Interrupts                        |
            |                        System Libraries                            |
            +--------------------------------------------------------------------+ --+
            |               System-Call interface to the Kernal                  |   |
            +-----------------------+------------------------+-------------------+   |
            | signal, terminal      | file system           | cpu scheduling     |   |
            | handling              | swapping block I/O    | page replacement   |   |--> Kernal
            | character I/O system  | system                | demand paging      |   |
            | terminal drivers      | disk and tape drivers | virtual memory     |   |
            +-----------------------+-----------------------+--------------------+   |
            |               Kernal interface to the Hardware                     |   |
            +--------------------------------------------------------------------+ --+
            | termial controllers   | device controllers    | memory controllers |   
            | terminals             | disk and tapes        | physical memory    |
            +-----------------------+-----------------------+--------------------+
        layered structure:
            Operating system divided into number of layers. lower most level or at level 0 there is hardware layer and the topmost layer is user interface
            This is easy to implement, debug and maintain also. The major advantage is that the hardware is protected from the layers above, unlike the simple structures the user interface can not directly access the hardware.
            Disadvantege is that to design layer structure as it is difficult to deside which layer should be top on a perticular layer and vice versa because only top layer can use below layer. This structure is not efficient than the other structures because one layer want to use the services provided by the layer below itthe request has to go down below each layer one by one and by the time service is actually provided. It may be late or it may not be very fast.

            +-------------------------------------------------------------------+
            |                   Layer N (User Interface)                        |
            |                                                                   |
            |       +-------------------------------------------------+         |
            |       |                        .                        |         |
            |       |                        .                        |         |
            |       |                        .                        |         |
            |       |                        .                        |         |
            |       |       +----------------------------------+      |         |
            |       |       |           Layer 1                |      |         |
            |       |       |      +--------------------+      |      |         |
            |       |       |      | Layer 0 (Hardware) |      |      |         |
            |       |       |      +--------------------+      |      |         |
            |       |       +----------------------------------+      |         |
            |       |                                                 |         |
            |       +-------------------------------------------------+         |
            |                                                                   |
            |                                                                   |
            |                                                                   |
            +-------------------------------------------------------------------+

        Microkernals:
            Instead of having a big kernal with so many functionalities, in this microkernal approcah we remove all non essential components from the kernal and we implement them as system and user level program.
            Microkernal just provide the core functionalities of the kernal and the other functionalities which are there like the device driver, the file server, process server, the virtual memory all these services are implemented as a user level or system program.
            The main function of the microkernal is to provide a communication between the client program and these system services or user level program.
            the communication between the client programs and the system programs, which provide the services are made through with something known as message passing.
            If a program is executing in a user mode even if that program crashes the entire system is not going to crash, but if it is running in the kernal mode and the perticular program fails the entire system is going to crash. so, in microkernal approach as most of the functionalities will run in user mode the crashing problem of the entire system is not going happen mostly. this is the advantage of microkernal approach.
            This also has its own disadvantages, like microkernal can suffer from performance decrease due to the increase system function overhead. Using message passing which helps in communication of the client programs and the system services there could be a system overhead, leading to a decreased performance.

                    +----------------------------------------------------------------+
                    |   +----------+      +---------+--------+---------+---------+   |
            user    |   | client   |      | device  | file   | process | virtual |   |
            mode    |   | program  | .....| drivers | server | server  | memory  |   |
                    |   +----------+      +---------+--------+---------+---------+   |
                    +----------------------------------------------------------------+
            kernal  |                         microkernal                            |
            mode    +----------------------------------------------------------------+
                    |                           hardware                             |
                    +----------------------------------------------------------------+
        
        Modules:
            Modules means we follow up modular approach in the structuring if operating system and this by far best methodology for operating system design which involves using object oriented program techniques to create a modular kernal.
            We have a core kernal and then this core kernal which have only the core functionalities of kernal and then the other functionalities are present in the form of modules which will be loaded to the kernal either at boot time or at run time. So, there are some functionalities like the device and bus drivers, scheduling classes, file system, loadable system calls, executable formats, stream module, and miscellaneous modules these modules dynamically loaded to the kernal as and when required.
            Its look like layer and microkernal approach but its better from those two. It resembles the layered system in such a way that each kernal section has defined protected interfaces. Each of the layer has a defined protected interface which are protected from the things that we dont want them to access. But it is more flexible than a layered system, where any module can call other directly instead if one layer wants to communicate with another layer it had to go through the layer all above it. Also we are having core kernal with only the core functionalities and then the other functions are loaded into it whenever necessary. But its advantage as compared to microkernal approach is that in microkernal approach, we need to have a message passing in order to communicate between the modules, because they are implemented as system or user level programs above the kernal. But in this one they are loaded dynamically directly into the core kernal as and when needed, so they doun't need to use message passing and hence the system overhead is not there.

                    +-----------------------+      +------------------------+
                    |    Device Drivers     |      |  Miscellaneous Modules |
                    +-----------------------+      +------------------------+
                                        |               |
                                        v               v
            +------------------+     +---------------------+     +---------------------+
            |  Bus Drivers     | --->|        Kernel       |<--- |  Scheduling Classes |
            +------------------+     |    (Core of OS)     |     +---------------------+
                                     |                     |
            +-------------------+    |                     |    +---------------------+
            | Loadable System   |--->|       Kernel        |<---|  Executable Formats |
            | Calls             |    |    (Core of OS)     |    +---------------------+
            +-------------------+    +---------------------+ 
                                        ^               ^
                                        |               |
                    +-----------------------+     +-----------------------+
                    |    Stream Modules     |     |    File System        |
                    +-----------------------+     +-----------------------+

    Kernal:
        following are the main tasks of kernal:
        1. Process Management
            The kernel manages processes (running programs), including creating, scheduling, and terminating them.
            It ensures that processes run efficiently and without interfering with each other.
        2. Memory Management
            The kernel manages system memory (RAM), allocating it to processes and ensuring they donâ€™t overwrite each otherâ€™s data.
            It uses techniques like virtual memory and paging to make the most efficient use of memory.
        3. Device Management
            The kernel controls and communicates with hardware devices like hard drives, printers, and network interfaces.
            It uses device drivers to manage input/output (I/O) operations, ensuring the software can interact with hardware.
        4. File System Management
            The kernel manages the file system, which stores and organizes files on storage devices (e.g., hard drives).
            It handles file creation, deletion, reading, and writing, and controls access to files based on permissions.
        5. Security and Access Control
            The kernel ensures the security of the system by enforcing user authentication and controlling permissions.
            It prevents unauthorized access to system resources and protects against malicious activities.

        Additional Important Kernel Tasks:
        Inter-process Communication (IPC)
            Facilitates communication between processes, allowing them to share data or synchronize their actions.
            Examples: message passing, shared memory, and semaphores.
        Network Management
            The kernel manages network connections and handles network protocols (like TCP/IP).
            Ensures data is transmitted securely and efficiently across the network.
        Scheduling
            The kernel determines how processes are scheduled for execution based on priority and resource availability.
            Uses scheduling algorithms (e.g., round-robin, priority-based) to manage process execution.
        Power Management
            The kernel manages power consumption by controlling how the CPU and devices use power.
            Implements sleep modes and energy-efficient tasks.
        Kernel Communication (System Calls)
            Provides a mechanism for user programs to request services from the kernel via system calls.
            Example: A user process may use a system call to interact with the file system or request memory.

    Unix is a multitasking, multi-user operating system that serves as the foundation for many modern operating systems, including Linux and macOS. Unix is the base for both Linux and macOS (Unix-like systems). Windows is a separate system that is not Unix-based, but has tools to run Linux apps (via WSL). Ubuntu is part of the Linux family and is a Unix-like system.
        

    1. Program Execution
    2. File system management
    3. Memory management
    4. Resource sharing
    5. Synchronization
    6. Convienience


Types of OS

1. Batch OS
    cpu contains (ALU- arthimetic logical unit) and control unit
    the CPU executes one job at a time; once a job completes or enters I/O wait, the CPU is idle until the next job is ready for execution.
    Idle CPU: The CPU remains idle during I/O operations as it only handles one job at a time.
    Starvation of Job/Process: Jobs may suffer from starvation if long or high-priority jobs keep getting processed first.
    
2. Multiprogramming OS

    Multi-Programming OS: Runs multiple jobs in memory simultaneously, utilizing CPU while one job waits for I/O.

    Batch OS vs. Multi-Programming OS:

    Batch OS: Executes one job at a time; CPU is idle during I/O.
    Multi-Programming OS: Switches to another job during I/O, reducing idle time. ( Advantage on batch os )

    Types:
    Time-Sharing OS: Shares CPU time among users/programs.
    Real-Time OS: Immediate response for time-critical tasks.

    Process Execution Scenario:
    In multi-programming, when the first process is halted due to an I/O wait, the CPU switches to the second process. When the first process's I/O is completed, what happens next depends on the scheduling algorithm used by the OS:

    Round-Robin/Preemptive: The CPU may switch back to the first process, even if the second hasnâ€™t finished, depending on the time slice allocated.
    Priority-Based/Shortest Job First: The OS may allow the second process to finish if it has higher priority or is shorter, before switching back to the first process.
    First-Come, First-Served: The OS might continue running the second process until it completes, then resume the first process.

3. multiprocessing OS
    Supports multiple processors (CPUs) working together to execute multiple processes simultaneously

    Key Features:
    Parallel Processing: Multiple CPUs( or cores) execute different processes/tasks at the same time, increasing system efficiency.
    Fault Tolerance: If one processor fails, others can take over.
    
    Types:
    Symmetric Multiprocessing (SMP): All processors share the same memory and OS.
    Asymmetric Multiprocessing (AMP): Each processor is assigned specific tasks, and only one runs the OS.

    In a multiprocessing OS, one CPU can handle multiple processes using time-sharing (also known as multitasking). This allows the CPU to switch between processes, giving the illusion that it is running more than one process simultaneously.

    However, whether a CPU runs 1 or more processes at a time depends on:

    CPU Cores: A single-core CPU can only run one process at a time but uses multitasking to switch between processes quickly. A multi-core CPU can run multiple processes in parallel.

    Time-Slicing (Scheduling): The OS scheduler decides how the CPU divides time between processes, so even a single CPU can appear to run multiple processes by switching between them rapidly.

    Process State: If a process is waiting for I/O, the CPU can switch to another process to avoid being idle.

    So, the actual parallelism depends on the number of CPU cores, but the OS manages time-slicing to handle multiple processes efficiently even on a single-core CPU.

4ï¸âƒ£ **Real-Time Operating System (RTOS)**
   - Processes data and responds within a strict time limit.
   - Used where timing is critical.
   - Example: Airbag system, medical devices, robotics, youtube streaming.
   - âš™ï¸ Types:
       - Hard RTOS â†’ Missed deadline = system failure.
       - Soft RTOS â†’ Occasional delay acceptable.

5ï¸âƒ£ **Distributed Operating System**
   - nodes in a network (lan, wan)
   - Manages multiple computers (nodes) as one system.
   - Tasks shared across connected machines.
   - Example: Amoeba, LOCUS, modern cloud clusters.
   - âš™ï¸ Features: Resource sharing, reliability, scalability.

6ï¸âƒ£ **Clustered Operating System**
   - nodes connected in lan network. 
   - Similar to distributed but tightly coupled.
   - Nodes work together for load balancing and high availability.
   - Example: Windows Server Clustering, Linux HA.
   - âš™ï¸ Features: Failover support, parallel processing.

7ï¸âƒ£ **Embedded Operating System**
   - Designed for small devices with limited resources.
   - Performs dedicated functions.
   - Example: Android (IoT), VxWorks, RTLinux, washingmachine, fridge.
   - âš™ï¸ Features: Compact, fast, low memory footprint.


4. Program(Secondary Memory - e.g. hdd) Vs Process(Main memory)
        process structure in main memory - [ 1.code 2.data 3.heap <-> 4.stack ]

5. Segementation Fault  (access out of the process boundaries )
        A segmentation fault (often called segfault) happens when a program tries to access memory that itâ€™s not allowed to. This could be because:
            It's accessing memory outside the range of what's allocated.
            It's using an invalid pointer (e.g., a null pointer).
        Essentially, the program is trying to read or write to a part of memory it shouldn't, and the operating system stops it for safety reasons.

6. A PCB (Process Control Block) 
        in an operating system is a data structure that contains all the information needed to manage a process. It includes:

        Process ID: Unique identifier for each process.
        Process State: Current state of the process (e.g., running, waiting, etc.).
        Program Counter: Address of the next instruction to be executed.
        CPU Registers: Values in the CPU registers for the process.
        Memory Management Information: Details like page tables, base, and limit registers.
        Priority: Process priority for scheduling.
        I/O Status Information: Information related to I/O devices.
        File List: List of files that the process has opened.
        Protection Information: Data for access control and permissions.
        These details help the OS manage processes, perform context switching, and ensure process execution and security.

7. Context Switch 
        In computing, registers, instructions, and the program counter work together to execute programs on a CPU:
            Registers: These are small, fast storage locations within the CPU used to hold data that is currently being processed. They store values such as operands for arithmetic operations, memory addresses, and intermediate results.
            Instructions: These are the commands that the CPU executes. Instructions are part of a program, telling the CPU what operations to perform, like arithmetic, data movement, or control flow.
            Program Counter (PC): This is a special register that holds the memory address of the next instruction to be executed in the program. After an instruction is executed, the PC is updated to point to the next instruction.
            Scheduler: Manages the order in which processes execute by allocating CPU time.
                types:
                    1. Long Term Schedular - (works in new <-> ready state) The long-term scheduler moves processes from the new state to the ready state based on their CPU and I/O time requirements, ensuring a balanced mix of CPU-bound(time) and I/O-bound(time) processes for optimal system performance.
                    2. Short-term scheduler (CPU scheduler): (works in ready -> run state) Selects processes from the ready queue and allocates CPU for execution based on scheduling algorithms.
                    3. Medium-term scheduler -  (process swapping scheduler): Swaps processes between main memory and suspended states (ready to suspended-ready, waiting to suspended-waiting) when there is not enough memory for high-priority processes; restores swapped processes after high-priority execution.

            Preemption: Forcibly interrupts a running process to give the CPU to another process, typically for better multitasking.


        Context switching is the process in an operating system where the CPU switches from executing one process to another. This allows for multitasking, where multiple processes can appear to run simultaneously, even though only one process can be actively executed by the CPU at a time.

        Steps in Context Switching:
            Saving the state: The current state of the running process (such as the program counter, register values, and memory mappings) is saved in the Process Control Block (PCB).
            Loading the next process: The operating system loads the state (from the PCB) of the next process to be executed.
            Resuming the next process: The CPU then resumes execution of the next process from where it left off.

8. Process State Transition 
        Process State Transition in an operating system describes how a process moves through different stages of its life cycle. The common states include:
            New â†’ The program is loaded into main memory from secondary storage.
            Ready â†’ The Process Control Block (PCB) is created, and the process is ready for execution.
            Running â†’ The CPU is actively executing the process.
            Waiting â†’ The process is waiting for an I/O operation to complete.
            Terminated â†’ The process has completed execution.

            +--------------------+
            |       New          | (created if its in secondary memory, if moved in main memory its new)
            +--------------------+
                  |
                  v
        +--------------------+
        |       Ready        | (PCB is ready for an process)
        +--------------------+
            |              ^
            v (execution)  |
  +----------------+       |
  |    Running     |----+  |
  +----------------+    |  |
         |              |  |  
         |              v  |
         |          +----------------------+   
         |          |      Waiting         | (I/O operations)
         |          +----------------------+
         |
         v
  +------------------+
  |    Terminated    |
  +------------------+

9. The degree of multiprogramming (DOM):
    It is the number of processes in the ready state. The Long-term scheduler (LTS) controls DOM by determining how many processes enter the system, while the Short-term scheduler (STS) has less direct control over DOM. Medium-term scheduling (MTS) reduces DOM by swapping processes in and out of memory.

10. Process Scheduling Terms:
        Arrival Time: The time when a process enters the ready queue.
        Waiting Time: The total time a process spends in the ready queue before getting CPU execution (time between ready and running).
        Burst Time: The time required for a process to execute on the CPU.
        Turnaround Time: The total time from process arrival to its completion, including execution and waiting time (completion time - arrival time).

11. CPU scheduling algorithms:
    1. First Come First Serve (FCFS): Non Preemptive

        Pno  | AT | BT | CT | TAT | WT    Pno = process no
        ------------------------------    AT  = arrival time
          1  |  0 |  3 |  3 |  3  |  0    BT  = burst time
          2  |  1 |  4 |  7 |  6  |  2    CT  = completion time
          3  |  2 |  2 |  9 |  7  |  5    TAT = turn around time  (CT - AT)
          4  |  3 |  1 | 10 |  7  |  6    WT  = waiting time

        Gantt Chart
        ------------------------------
        P1  | P2  | P3  | P4 |
        ------------------------------
        0   3     7     9    10


        TAT = CT - AT
        WT  = TAT - BT
        CT (Traverse Right to Left)
    
    2. Shortest Job First (SJF): Non Preemptive


        Pno  | AT | BT | CT | TAT | WT
        ------------------------------
          1  |  0 |  3 |  3 |  3  |  0
          2  |  1 |  4 | 10 |  9  |  5
          3  |  2 |  2 |  6 |  4  |  2
          4  |  3 |  1 |  4 |  1  |  0

        Gantt Chart
        ------------------------------
        P1  | P4  | P3  | P2  |
        ------------------------------
        0   3     4     6     10

    3. Round Robin

        Quantum Time (QT) = Quantum time in Round Robin scheduling refers to the fixed time slice allocated to each process for execution before it is preempted and moved back to the ready queue, ensuring fair CPU time distribution among all processes.

        TQ = 2

        Pno  | AT | BT | CT | TAT | WT
        ------------------------------
          1  |  0 |  3 |  7 |  7  |  4
          2  |  1 |  4 | 10 |  9  |  5
          3  |  2 |  2 |  6 |  4  |  2
          4  |  3 |  1 |  8 |  5  |  4

        Gantt Chart
        -------------------------------------------
        P1  | P2  | P3  | P1  | P4  | P2  |
        -------------------------------------------
        0   2     4     6     7     8     10

ðŸ§  MULTILEVEL QUEUE SCHEDULING (Operating System)
-------------------------------------------------------------

ðŸŽ¯ **Definition:**
Multilevel Queue Scheduling is a CPU scheduling method that divides the ready queue 
into **multiple separate queues**, each with its own **priority level** and **scheduling algorithm**.

-------------------------------------------------------------
ðŸ—ï¸ **Concept:**

- Processes are permanently assigned to one queue based on their type or priority.
- Each queue has its own scheduling policy.
- The CPU is allocated to queues based on their priority order.

-------------------------------------------------------------
ðŸ§© **Example Queue Structure:**

| Queue Level  | Process Type         | Scheduling Algorithm          |
|--------------|----------------------|-------------------------------|
| Q1 (Highest) | System / Real-time   | Round Robin (RR)              |
| Q2           | Interactive          | Shortest Job First (SJF)      |
| Q3           | Batch / Background   | First Come First Serve (FCFS) |
-----------------------------------------------------------------------

ðŸ’¡ **Advantages:**
- Simple and organized.
- Suitable when process types are distinct (system vs user jobs).
- Predictable CPU allocation per queue.

âš ï¸ **Disadvantages:**
- Rigid â€” processes cannot move between queues.
- Can lead to **starvation** of lower-priority queues.
- Not adaptive to changing process behavior.

-------------------------------------------------------------

ðŸ§  MULTILEVEL FEEDBACK QUEUE (MLFQ) SCHEDULING
-------------------------------------------------------------

ðŸŽ¯ **Definition:**
Multilevel Feedback Queue (MLFQ) is an advanced CPU scheduling algorithm that allows
**processes to move between multiple queues** based on their **behavior and CPU usage**.

Itâ€™s an improvement over **Multilevel Queue Scheduling** (where queues are fixed).

-------------------------------------------------------------
ðŸ—ï¸ **Concept:**
- The ready queue is divided into multiple levels.
- Each level (queue) has its own **priority** and **time quantum**.
- A process can move **up or down** between queues depending on how much CPU time it uses.

-------------------------------------------------------------
ðŸ§© **Example Queue Structure:**

| Queue Level | Priority | Time Quantum | Scheduling Type |
|--------------|-----------|---------------|------------------|
| Q1 (Top)     | Highest   | 10 ms         | Round Robin (RR) |
| Q2           | Medium    | 20 ms         | Round Robin (RR) |
| Q3 (Bottom)  | Lowest    | FCFS          | Batch / FCFS     |

-------------------------------------------------------------
âš™ï¸ **How It Works:**

1ï¸âƒ£ **New processes** start in the top-priority queue (Q1).  
2ï¸âƒ£ If a process **uses up its entire time quantum**, it is **moved to a lower queue**.  
3ï¸âƒ£ If a process **waits too long**, it may be **promoted to a higher queue** (to avoid starvation).  
4ï¸âƒ£ The scheduler always picks the **highest-priority non-empty queue**.

-------------------------------------------------------------
ðŸ’¡ **Advantages:**
âœ… Reduces starvation (can promote processes)  
âœ… Adapts to process behavior (I/O bound vs CPU bound)  
âœ… Efficient and fair for mixed workloads  

âš ï¸ **Disadvantages:**
âŒ Complex to implement  
âŒ Hard to tune time quantum and promotion/demotion rules  

-------------------------------------------------------------

12. Critical section and Race condition:
        Critical Section: A part of a program where shared resources are accessed, such as a shared variable count. Only one process should be in its critical section at any time to avoid incorrect behavior.
        Race Condition: Occurs when two or more processes execute concurrently and try to modify the shared variable at the same time, leading to inconsistent results.

        count = 10 (critical section or shared memory)
        Process P1 (Increment): 1) Read: R1 = count 2) Increment: R1 = R1 + 1 3) Write: count = R1
        Process P2 (Decrement): 1) Read: R3 = count 2) Decrement: R3 = R3 - 1 3) Write: count = R3

        Scenario 1:
        P1: 1, 2, 3 (R1 = 11) | P2: 1, 2, 3 (R2 = 10) => count = 10

        Scenario 2:
        P1: 1, 2 (R1 = 11) | P2: 1, 2, 3 (R2 = 9) | P1: 3 => count = 11

        Scenario 3:
        P1: 1, 2 (R1 = 11) | P2: 1, 2 (R2 = 9) | P1: 3 | P2: 3 => count = 9

13. Synchronization Conditions:
        Mutual Exclusion: Ensures that only one process can enter its critical section (where shared resources are accessed) at a time. This prevents race conditions, as no two processes can access or modify shared data simultaneously.
        Progress: ( If process P1 is not interested in entering the critical section, it should not block or prevent process P2 from entering the critical section.) Guarantees that if no process is in its critical section and some processes wish to enter, one of the waiting processes will eventually be allowed to enter. This ensures that the system doesn't become idle unnecessarily, and work continues.
        Bounded Waiting: (If process P1 wants to enter the critical section multiple times, it should not indefinitely prevent process P2 from entering. P1 should ensure that P2 waits for a bounded amount of time before getting its turn.) Ensures that every process has a bound (limit) on the number of times other processes are allowed to enter their critical sections before it can enter its own. This prevents starvation, ensuring that every process eventually gets its turn. (starvation = finite waiting, deadlock - infinite waiting)
        Architectural Neutrality / Portability: Implies that synchronization solutions (like algorithms or mechanisms) should work across different hardware architectures without needing major changes. The implementation should be portable and not tied to a specific hardware platform.

14. Synchronization Mechanisms:
        1. Busy Waiting (Spinlock): A process continuously checks if it can enter the critical section, waiting until another process finishes using it. This consumes CPU cycles while waiting.
            1. Lock Variable
            2. Test and Set LOCK (TSL)
            3. Turn Variable
            4. Peterson method
        2. Without Busy Waiting: A process goes into a sleep state until another process finishes using the critical section. Once the critical section is available, the sleeping process is woken up, saving CPU utilization.
            1. Sleep and wake up.

    1. Lock Variable:
        Software Mechanism (user mode)
        works for more than 2 processes
        mutual exclusion not garented
       +----------------------+
       | 1. while (LOCK != 0);|
       | 2. LOCK = 1;         |
       +----------------------+
        CS (critical section)
        +------------+
        |3. LOCK = 0;|
        +------------+                                                          # Idle case                                                     # Mutual Exclusion not follows

        Steps :                                                                  Time  |  P1                      | P2                          Time   | P1                      | P2
            1) Load LOCK, R0                                                    ------------------------------------------------                ------------------------------------------------
            2) CMP R0, #0             (CMP = compare)                           T0     | while (LOCK != 0)        |                             T0     | Load LOCK, R0            | Load LOCK, R0
            3) JNZ 1)                 (JNZ = jump not equal to 0)               T1     | LOCK = 1 (enter CS)      | while (LOCK != 0)           T1     | CMP R0, #0 (LOCK = 0)    | CMP R0, #0 (LOCK = 0)
            4) Store #1, LOCK                                                   T2     | Critical Section         |                             T2     | Store #1 in LOCK (LOCK=1)| Store #1 in LOCK (LOCK=1)
            .                                                                   T3     | LOCK = 0 (exit CS)       | LOCK = 1 (enter CS)         T3     | Enter Critical Section   | Enter Critical Section
            .                                                                   T4     |                          | Critical Section            T4     | Critical Section         | Critical Section
            critical section                                                    T5     |                          | LOCK = 0 (exit CS)          T5     | LOCK = 0 (exit CS)       | LOCK = 0 (exit CS)

    2. Test and Set LOCK (TSL):
            mutual exclusion and progress are garented, but Bounded Waiting and Architectural Neutrality not garented

           +-------------------+                                                        Time   | P1                       | P2
           | 1) Load LOCK, R0  |   --->   TSL LOCK, R0  (atomic instruction)            ------------------------------------------------
           | 4) Store #1, LOCK |                                                        T0     | TSL LOCK, R0             | 
           +-------------------+                                                        T1     | LOCK = 1 (enter CS)      | TSL LOCK, R0 (blocked)
            2) CMP R0, #0             (CMP = compare)                                   T2     | Critical Section         | 
            3) JNZ 1)                 (JNZ = jump not equal to 0)                       T3     | LOCK = 0 (exit CS)       | LOCK = 1 (enter CS)
            .                                                                           T4     |                          | Critical Section
            .                                                                           T5     |                          | LOCK = 0 (exit CS)
            critical section                                            
    
    3. Turn Variable:
            Software mechanism (user mode)
            works for only 2 process
            Mutual exclusion garented but Progress not garented.
                            Turn = 0
                P0              |       P1
            while(turn != 0);   |   while(turn != 1);
            critical section    |   critical section
            turn = 1            |   turn = 0
            :                   |   :
            :                   |   :
    
    4. Peterson method
            works for only 2 process                                            Time   | P0 (turn = 0)                 | P1 (turn = 1)
            process         0   |   1                                           -----------------------------------------------------
            interested      F   |   F                                           T0     | interested[0] = true          | Waiting for turn
            turn            0   |   1                                           T1     | turn = 1 (give chance to P1)  | Waiting for turn
            other           1   |   0                                           T2     | While loop: P0 checks P1's    | interested[1] = true
                                                                                       | interest and waits (P1's turn)| turn = 1 (remains P1's turn)
            Entry Section(process){                                             T3     | Waiting for turn              | Enter Critical Section
                1. int other;                                                   T4     | Waiting for turn              | Critical Section
                2. other = 1 - process;                                         T5     | Waiting for turn              | interested[1] = false (exit CS)
                3. interested[process] = true;                                  T6     | Enter Critical Section        | Waiting for P0
                4. turn = process                                               T7     | Critical Section              | Waiting for P0
                5. while(interested[other] = true && turn = process);           T8     | interested[0] = false (exit CS)| Waiting for turn
            }

            critical section

            exit section(process){
                6. interested[process] = false;
            }
    
    1. Sleep and Wake up:

        Buffer[] of size N
        and count for an item in b

        Producer(){                                 Consumer(){    
            int item;                                   int item;
            while (true){                               while(true){
                item = produce_item();                      if(count == 0)
                if(count == N);                                 sleep();
                    sleep();                                item = remove_item();
                insert_item(item);                          count--;
                count++;                                    if(count == N-1)
                if(count==1)                                    wake_up(Producer);
                    wake_up(Consumer);                      consume_item(item)
            }                                           }
        }                                           }

15. Deadlock
        A deadlock is a situation in an operating system where two or more processes are unable to proceed because each is waiting for the other to release resources.

            1. Mutual Exclusion: Only one process can hold a resource at a time.
            2. Hold and Wait: A process holding at least one resource is waiting for additional resources that are held by other processes.
            3. No Preemption: Resources cannot be forcibly taken from a process; they must be released voluntarily.
            4. Circular Wait: A closed chain of processes exists where each process holds a resource the next one needs.

                +----------+         +----------+        
                |Process 1 | ------> |Resource A|           Process 1 holds Resource A and is waiting for Resource B.
                +----------+         +----------+           Process 2 holds Resource B and is waiting for Resource C.
                   ^                      |                 Process 3 holds Resource C and is waiting for Resource A.
                   |                      v
                +----------+         +----------+        
                |Resource C| <------ |Process 3 |
                +----------+         +----------+
                    ^                      |
                    |                      v
                +----------+         +----------+        
                |Process 2 | <------ |Resource B|
                +----------+         +----------+

        Deadlock Avoidance: Banker's Algorithm
            The Banker's Algorithm is a resource allocation and deadlock avoidance algorithm that tests for the safety of the system before allocating resources to processes. It operates by ensuring that resources are only allocated in such a way that the system remains in a safe state.

            1. Safe state:

                (Allocated)             (Requested)                                             P1 (fullfillment first according resource available)    P0                  P2
                    R0  R1  R2              R0  R1  R2                      R0  R1  R2          2   0   1                                               1   2   1           2   2   1
                P0  1   2   1           P0  1   0   3           Total       5   5   5       +   0   1   2                                           +   2   1   3       +   3   3   4
                P1  2   0   1           P1  0   1   2           Allocated   5   4   3           ---------                                               ---------           ---------
                P2  2   2   1           P2  1   2   0           Available   0   1   2           2   1   3                                               3   3   4           5   5   5
            
            2. Usafe state:

                (Allocated)             (Requested)                                             
                    R0  R1  R2              R0  R1  R2                      R0  R1  R2          
                P0  1   2   2           P0  1   0   2           Total       5   5   5       
                P1  2   0   1           P1  0   1   2           Allocated   5   4   4         
                P2  2   2   1           P2  1   2   0           Available   0   1   2       

16. Relocation
        Relocation refers to the process of adjusting program addresses at runtime to account for the program's location in memory. Since a program may not always load into the same physical memory location, the operating system needs to translate logical addresses (used in the code) into physical addresses (actual locations in memory).

        Why Relocation is Needed:
            Dynamic Loading: Programs do not always load into the same memory address.
            Memory Management: To efficiently use memory and avoid fragmentation, processes are relocated in different areas of memory.
            Swapping: Processes are moved in and out of memory during execution.

                Secondary Memory                                    Main Memory
        +----------------------------------+              0 +-------------------------+
        |1  (program instruction address)  |  ->            | Offset added at runtime |
        |2                                 |  ->            | Relocation done by MMU  |
        |3                                 |            10k +-------------------------+
        |4   jump [1]                      |                |1                        |
        |5                                 |                |2                        |
        |6                                 |                |3                        |
        |7                                 |                |4 Jump [10k + 1]         |
        |8                                 |----->          |5                        |
        +----------------------------------+                |6                        |
                                                            |7                        |
             Relocatable address                            |8                        |
                                                            +-------------------------+
                                                            |                         |
                                                            |                         |
                                                            +-------------------------+

                                                                Absolute address

Memory types - Registers, cache, ram, secondary memory, tertiory memory                                                                

17. Contiguous Memory Allocation:
        
        Fragmentation
            1. Internal Fragmentation: Occurs in fixed partitioning. Internal fragmentation happens when a partition is larger than the process's memory requirement, leaving unused space within the allocated memory block.
            2. External Fragmentation: Occurs in variable partitioning. External fragmentation happens when free memory exists but is not contiguous, preventing processes that require large contiguous memory blocks from being allocated.
        
        There are two main types of partitioning in contiguous memory allocation:
            1. Fixed Partitioning (Internal and External Fragmentation) - partionin size is fixed
                    +-----------+---------+---------------+---------------+
                    | Partition |   Size  |   Process      | Unused Space |
                    +-----------+---------+---------------+---------------+
                    |    P1     |  4 MB   | P1 (2 MB)      |   2 MB       |
                    +-----------+---------+---------------+---------------+
                    |    P2     |  4 MB   | P2 (4 MB)      |   0 MB       |
                    +-----------+---------+---------------+---------------+
                    |    P3     |  4 MB   | P3 (1 MB)      |   3 MB       |
                    +-----------+---------+---------------+---------------+
                    |    P4     |  4 MB   | P4 (3 MB)      |   1 MB       |
                    +-----------+---------+---------------+---------------+
                limits: 
                    1. internal fragmentation. ( unused space in one partion)
                    2. limit on process size.
                    3. limit on degree of multiprograming.
                    4. external fragmentation (unused space through the memory).

            2. Variable Partitioning (External Fragmentation)
                    +-----------+---------+---------------+---------------+
                    | Partition |   Size  |   Process      | Free Space   |
                    +-----------+---------+---------------+---------------+
                    |    P1     |  2 MB   | P1             |   0 MB       |
                    +-----------+---------+---------------+---------------+
                    |    P2     |  4 MB   | P2             |   0 MB       |
                    +-----------+---------+---------------+---------------+
                    |    P3     |  6 MB   | P3             |   0 MB       |
                    +-----------+---------+---------------+---------------+
                    |    P4     |  3 MB   | P4             |   0 MB       |
                    +-----------+---------+---------------+---------------+
                
                1. external fragmentation.
                2. Allocation/Deallocation is complex.

            Compaction Method : reduces CPU utilization

                Before Compaction:  Free space is fragmented across multiple partitions
                    
                    +-----------+---------+---------------+---------------+
                    | Partition |   Size  |   Process      | Free Space   |
                    +-----------+---------+---------------+---------------+
                    |    P1     |  2 MB   | P1             |   0 MB       |
                    +-----------+---------+---------------+---------------+
                    |           |  3 MB   | Free           |   3 MB       |
                    +-----------+---------+---------------+---------------+
                    |    P2     |  4 MB   | P2             |   0 MB       |
                    +-----------+---------+---------------+---------------+
                    |           |  2 MB   | Free           |   2 MB       |
                    +-----------+---------+---------------+---------------+
                    |    P3     |  6 MB   | P3             |   0 MB       |
                    +-----------+---------+---------------+---------------+
                    |    P4     |  3 MB   | P4             |   0 MB       |
                    +-----------+---------+---------------+---------------+

                After compaction:   Free space is consolidated into a larger contiguous block after moving processes together, reducing external fragmentation.

                    +-----------+---------+---------------+---------------+
                    | Partition |   Size  |   Process      | Free Space   |
                    +-----------+---------+---------------+---------------+
                    |    P1     |  2 MB   | P1             |   0 MB       |
                    +-----------+---------+---------------+---------------+
                    |    P2     |  4 MB   | P2             |   0 MB       |
                    +-----------+---------+---------------+---------------+
                    |    P3     |  6 MB   | P3             |   0 MB       |
                    +-----------+---------+---------------+---------------+
                    |    P4     |  3 MB   | P4             |   0 MB       |
                    +-----------+---------+---------------+---------------+
                    |           |  5 MB   | Free           |   5 MB       |
                    +-----------+---------+---------------+---------------+

            To avoid external fragmentation, non-contiguous memory allocation techniques like Paging and Segmentation are used, where memory is divided into fixed-size pages or variable-sized segments, eliminating the need for large contiguous memory blocks.
            
            Allocation and Deallocation of Memory into Processes and Holes
                In dynamic memory allocation, processes and holes (free memory blocks) alternate as memory is allocated and deallocated. Let's look at an example. When a process is deallocated, it leaves behind a hole, or a free block of memory. These holes can be used by other processes, but managing them properly is crucial to avoid fragmentation.
                
            Initial State (Full Memory Available)
            +-----------+---------+
            | Free (10 MB)        |
            +-----------+---------+

            After Allocating 3 Processes (P1, P2, P3)
            +-----------+---------+---------------+---------------+
            |  P1       |  3 MB   | Free          |   1 MB        |
            +-----------+---------+---------------+---------------+
            |  P2       |  4 MB   |               |               |
            +-----------+---------+---------------+---------------+
            |  P3       |  2 MB   |               |               |
            +-----------+---------+---------------+---------------+

            After Allocating P4 (Fills the Hole)
            +-----------+---------+---------------+---------------+
            |  P1       |  3 MB   | P4            |   4 MB        |
            +-----------+---------+---------------+---------------+
            |  P4       |  4 MB   |               |               |
            +-----------+---------+---------------+---------------+
            |  P3       |  2 MB   |               |               |
            +-----------+---------+---------------+---------------+

            Holes are created when processes are deallocated. In this case, when P2 is deallocated, it leaves behind a hole of 4 MB.
            Holes are filled when new processes are allocated (in this case, P4).
        
        Bitmap Allocation
            In bitmap allocation, memory is divided into fixed-size allocation units. A bitmap is used to track whether each unit is free or allocated.

            1 bit = Allocated (used by a process)
            0 bit = Free (available for allocation)
        
        +-----------+---------+-------------------+---------------+
        | Partition |   Size  |   Process          | Free Space   |
        +-----------+---------+-------------------+---------------+
        |    P1     |   3 MB  |   [1][1][1]        |              |
        +-----------+---------+-------------------+---------------+
        |    P2     |   4 MB  |   [1][1][1][1]     |              |
        +-----------+---------+-------------------+---------------+
        |    P3     |   2 MB  |   [1][1]           |              |
        +-----------+---------+-------------------+---------------+
        |   Free    |   1 MB  |                   |   [0]         |
        +-----------+---------+-------------------+---------------+

        Bitmap: [1, 1, 1, 1, 1, 1, 1, 1, 0]

18. Memory Management using Linked list:
        In OS memory management using linked lists, each node represents either a process or a hole, along with the start and end memory addresses. Here's an example of a single linked list showing memory blocks with a process (P) and a hole (H):

        Single Linked List 
        +----+--------+--------+  -->  +----+--------+--------+  -->  +----+--------+--------+
        | P  |   10   |   17   |       | H  |   18   |   25   |       | P  |   26   |   35   |
        +----+--------+--------+       +----+--------+--------+       +----+--------+--------+

        Double Linked List
        <---+----+--------+--------+--->  <----+----+--------+--------+--->  <---+----+--------+--------+--->
            | P  |   10   |   17   |           | H  |   18   |   25   |          | P  |   26   |   35   |
            +----+--------+--------+           +----+--------+--------+          +----+--------+--------+

    In OS linked list memory management, the methods for allocating memory are:

        1. First Fit: The system scans the linked list and allocates the first available hole that is large enough for the process.
                Advantage: Fast as it stops at the first sufficient hole.
                Disadvantage: Can cause fragmentation at the start of memory.

        2. Next Fit: Similar to First Fit, but it continues searching from the last allocated position.
                Advantage: Reduces the likelihood of repeatedly searching the same areas.
                Disadvantage: Can leave small unused fragments.

        3. Best Fit: The system scans the entire list and selects the smallest hole that fits the process.
                Advantage: Minimizes wasted space in holes.
                Disadvantage: Can lead to many small unusable holes (external fragmentation).

        4. Worst Fit: Allocates the largest available hole.
                Advantage: Leaves larger holes for future processes.
                Disadvantage: Might waste large amounts of memory.

        +----+--------+ -> +----+--------+ -> +----+--------+ -> +----+--------+ -> +----+--------+ -> +----+--------+ -> +----+--------+ -> +----+--------+ -> +----+--------+ -> +----+--------+ 
        | H  |   30   |    | P  |   50   |    | H  |   40   |    | P  |   60   |    | H  |   20   |    | P  |   70   |    | H  |   25   |    | P  |   80   |    | H  |   35   |    | P  |   45   |  
        +----+--------+    +----+--------+    +----+--------+    +----+--------+    +----+--------+    +----+--------+    +----+--------+    +----+--------+    +----+--------+    +----+--------+ 

19. Pagination
        Pages: Fixed-size blocks of the process in Secondary memory.
        Frames: Fixed-size blocks in ram.
        Page size = Frame size

        +-----+     +-----+     +-----+     +-----+
        | P1  |     | P2  |     | P3  |     | P4  |
        | P1  |     | P2  |     | P3  |     | P4  |
        | P1  |     +-----+     | P3  |     | P4  |
        +-----+       P2        | P3  |     +-----+
          P1                    | P3  |       P4
                                +-----+
                                  P3

        Main Memory:

           +-----+             +-----+                     +-----+
       F1  | P1  | Page 1      | P1  | Page 1              | P1  | Page 1
       F2  | P1  | Page 2      | P1  | Page 2              | P1  | Page 2
       F3  | P1  | Page 3      | P1  | Page 3              | P1  | Page 3
           +-----+             +-----+                     +-----+
       F4  | P2  | Page 4      | H   | Hole (2 pages)      | P5  | Page 4
       F5  | P2  | Page 5      | H   |                     | P5  | Page 5
           +-----+             +-----+                     +-----+
       F6  | P4  | Page 6      | P4  | Page 6              | P4  | Page 6
       F7  | P4  | Page 7      | P4  | Page 7              | P4  | Page 7
       F8  | P4  | Page 8      | P4  | Page 8              | P4  | Page 8
       F9  | P4  | Page 9      | P4  | Page 9              | P4  | Page 9
       F10 | P4  | Page 10     | P4  | Page 10             | P4  | Page 10
           +-----+             +-----+                     +-----+
       F11 | P3  | Page 11     | H   | Hole (3 pages)      | P5  | Page 11
       F12 | P3  | Page 12     | H   |                     | P5  | Page 12
       F13 | P3  | Page 13     | H   |                     | P5  | Page 13
           +-----+             +-----+                     +-----+

    Process P1 (3 pages), P2 (2 pages), and P4 (5 pages) are allocated.

    Initial Allocation for P1 (3 pages)

19. Memory Management Unit (MMU) Explanation:
        The Memory Management Unit (MMU) is a hardware component responsible for translating logical addresses into physical addresses. It helps manage virtual memory and ensures that processes can access memory efficiently and securely.
        Logical Address: Generated by the CPU and contains the page number and offset. This address is virtual and needs translation into a physical address.
        Physical Address: Corresponds to the actual location in main memory. It contains the frame number and offset, after translating from the logical address.
        Logical and Physical Address Breakdown:
        Logical Address = Page Number + Offset
        Physical Address = Frame Number + Offset
        The page number maps to a frame number in physical memory, and the offset remains the same in both logical and physical addresses.

        +---------+---------+       +---------+---------+
        | Page No | Offset  |       | Frame No| Offset  |
        +---------+---------+       +---------+---------+
        |  P1-0   |   12    |       |   3     |   12    |
        +---------+---------+       +---------+---------+
        |  P3-2   |   34    |       |   4     |   34    |
        +---------+---------+       +---------+---------+
        |  P4-5   |   45    |       |   5     |   45    |
        +---------+---------+       +---------+---------+  

20. Page Table:
        The page table is a data structure used in virtual memory systems to map logical (virtual) addresses to physical addresses. Each process has its own page table, which contains page table entries (PTEs) that store the mapping from virtual pages to physical frames.

        Page Table Entries (PTEs):
            A Page Table Entry includes information such as the frame number in physical memory, status bits (like whether the page is in memory), and permissions (read/write access).
            Page Table Entries consists of:
                1. Reference bit (Accessed bit):
                    Indicates: Whether the page has been accessed (read or written) in last clock cycle by cpu for algorithms like least recently use.
                    1 = Accessed, 0 = Not accessed.
            
                2. P/A (Physical Address):
                    Indicates: Whether the page has been present or absent in main memory due to demand paging. 
                    Stored as: P = present, A = absent.

                3. Modified bit (Dirty bit):
                    Indicates: Whether the page has been modified (written to).
                    1 = Modified, 0 = Not modified.

                4. Protection bits:
                    Indicates: The access rights for the page (e.g., read/write, execute, user/kernel).
                    Read/Write: 1 = Read/Write, 0 = Read-only.
                    User/Supervisor: 1 = User, 0 = Supervisor (kernel).

                5. Frame Number:
                    Indicates: The physical page frame address (high-order bits in the PTE).

        Page Table Base Register (PTBR):
            The Page Table Base Register (PTBR) holds the starting address of the page table in memory. When the CPU generates a logical address, it uses the PTBR to locate the corresponding page table for the current process.

            
           +---------+               +-----------------+           +-----------------+
           |  CPU    |               | Page Table Base |           |  Main Memory    |
           | Logical | --(LA)------> | Register (PTBR) |           +-----------------+
           | Address |               +-----------------+           |     OS          |
           +---------+                              |          +---+-----------------+
                                                    |          | P1(1) - Frame 3     |
          +----------------------------------+      |          +---------------------+
          |             Page Table           |      |          | P1(2) - Frame 4     | <------+
          +----------------------------------+      |          +---------------------+        |
          |  Page No.  |  Frame No.          |      |          | P1(Page Table)      |        |
          +------------+---------------------+      +--------> | Frame 7 (PTBR)      |----+   |
          |     0      |      3              |                 +---------------------+    |   |
          |     1      |      4              | <------------------------------------------+   |
          |     2      |      7 (PTBR)       | ------------(PA)-------------------------------+                                      
          +----------------------------------+          

            Logical address = P1(page - 2 & offset - 5)
            Page table base register = page table -> frame 7
            page table = logical address -> physical address (Frame - 4 & offset - 5)

    Multilevel Paging:
        Multilevel Paging in OS is a memory management scheme that avoids the limitations of a single-level page table, especially when the virtual address space is large. Multilevel paging uses multiple levels of page tables, where the first-level page table points to second-level page tables, and so on.

        +---------+               +-----------------+           +-------------------------------------------------------+
        |  CPU    |               | Page Table Base |           | Main Memory                                           |
        | Logical | --(LA)------> | Register (PTBR) |           +-------------------------------------------------------+
        | Address |               +-----------------+           |     OS                                                |
        +---------+                              |              +-------------------------------------------------------+
            +------------------------------------+              | Frame 3 - P1 (Page 0)                                 |
            |     +----------------------------------+          +-------------------------------------------------------+
            |     |        First-Level Page Table    |          | Frame 4 - P1 (Page 1)                                 |
            |     +----------------------------------+  (PA)    +-------------------------------------------------------+
            |     |  Page No.  |  Second-Level PT    |------->  | Frame 5 - P1 (Page 2)                                 | 
            |     +------------+---------------------+          +-------------------------------------------------------+
            |     |     0      |      Frame 9        |          | Frame 6 - P1 (Page 3)                                 |           
            |     |     1      |      Frame 10       |          +-------------------------------------------------------+
            |     |     2      |      Frame 11       |          | Frame 7 - P1 (Page 4)                                 |
            |     +----------------------------------+          +-------------------------------------------------------+
            |                            ^                      | Frame 8 - P1 (Page 5)                                 |  
            |                            |                      +-------------------------------------------------------+    
            |                            |                      | Frame 9 Second-Level Page Table (Page 1)              |  
            |     +----------------------------------+          +-------------------------------------------------------+
            +---> |       Second-Level Page Table    |          | Frame 10 Second-Level Page Table (Page 2)             |
                  +----------------------------------+          +-------------------------------------------------------+
                  |  Page No.  |  Frame No.          |          | Frame 11 Second-Level Page Table (Page 3)             |
                  +------------+---------------------+          +-------------------------------------------------------+
          Page 0  |     0      |      3              |          | Frame 12 First-Level Page Table (PTBR)                |
                  |     1      |      4              |          +-------------------------------------------------------+
                  +----------------------------------+          |                                                       |
          Page 1  |     2      |      5              |          +-------------------------------------------------------+                                                       
                  |     3      |      6              |  
                  +----------------------------------+        
          Page 2  |     4      |      7              |
                  |     5      |      8              |                                                                
                  +----------------------------------+          

ðŸ§  INVERTED PAGING (Inverted Page Table) â€” 

----------------------------------------------------
ðŸ“˜ Definition:
Inverted Paging uses a single page table for the entire system
instead of having one page table per process.

Each entry in the table corresponds to a physical frame
and stores (Process ID, Virtual Page Number, Control bits).

----------------------------------------------------
ðŸ“— Structure of an Inverted Page Table Entry:
| Frame No | PID | Virtual Page | Control Bits |

----------------------------------------------------
ðŸ“˜ Working (Address Translation Steps):
1. CPU generates a virtual address (VPN, Offset).
2. OS searches the Inverted Page Table using (PID, VPN).
3. If found â†’ retrieves the corresponding Physical Frame Number (PFN).
4. Combine PFN + offset â†’ gives the physical address.
5. If not found â†’ Page Fault occurs â†’ page loaded from disk.

----------------------------------------------------
ðŸ“— Example:
| Frame No | PID | Virtual Page | Valid |
|-----------|-----|---------------|--------|
| 0 | 3 | 5 | 1 |
| 1 | 2 | 4 | 1 |
| 2 | 1 | 9 | 1 |
| 3 | 3 | 7 | 1 |

If Process 3 requests Virtual Page 7 â†’ found in Frame 3.

----------------------------------------------------
ðŸ“˜ Advantages:
âœ… Reduces memory usage (only one table for the system)
âœ… Efficient for large virtual address spaces (like 64-bit)
âœ… Scales better than per-process tables

----------------------------------------------------
ðŸ“™ Disadvantages:
âŒ Slower lookup (must search or use hashing)
âŒ More complex to manage in hardware

----------------------------------------------------
ðŸ“— Comparison:

| Feature             | Normal Page Table | Inverted Page Table |
|----------------------|------------------|---------------------|
| No. of Tables        | One per process  | One for entire system |
| Indexed By           | Virtual Page No. | Physical Frame No.  |
| Memory Usage         | Large            | Small               |
| Lookup Speed         | Fast (direct)    | Slow (search/hash)  |

----------------------------------------------------
ðŸ“˜ Optimization:
To reduce lookup time â†’ systems often use a HASHED PAGE TABLE.
----------------------------------------------------------------------------------

ðŸ§  OVERLAY STRUCTURE IN OPERATING SYSTEM 

----------------------------------------------------
ðŸ“˜ DEFINITION:
Overlay is a memory management technique used to run
large programs in small main memory.

ðŸ‘‰ Only the required part (module) of a program is loaded into memory at a time.
ðŸ‘‰ Unused parts are swapped out and replaced (overlaid) by other modules when needed.

----------------------------------------------------
ðŸ“— PURPOSE:
- Used when program size > available main memory.
- Saves memory by keeping only the active part in RAM.
- Implemented by the programmer (in older systems).

----------------------------------------------------
ðŸ“˜ BASIC CONCEPT:
Program is divided into independent *modules* (segments).
Some modules are permanently resident, while others are overlaid.

Example:
sql
Copy code
    +---------------------------+
    |       MAIN ROUTINE        | <-- Resident (always in memory)
    +---------------------------+
    |       OVERLAY 1           | <-- Loaded when needed
    +---------------------------+
    |       OVERLAY 2           | <-- Loaded after Overlay 1
    +---------------------------+
sql
Copy code

When Overlay 2 is required, Overlay 1 is replaced (overlaid) in memory.

----------------------------------------------------
ðŸ“™ COMPONENTS OF OVERLAY STRUCTURE:

1ï¸âƒ£ **Symbol Table**
   - Contains names and addresses of variables, functions, and labels.
   - Used by the linker/loader to resolve references across overlays.

2ï¸âƒ£ **Common Routine (Shared Code)**
   - Part of the program used by multiple overlays.
   - Loaded permanently in memory (so overlays can call it when needed).

3ï¸âƒ£ **Pass (Compiler/Assembler Phase)**
   - The process of generating object code from source code.
   - In overlay systems, passes are divided so that each pass fits into memory.
   - Example: 
     - Pass 1: Generates symbol table.
     - Pass 2: Generates final machine code using symbol table.

4ï¸âƒ£ **Overlay Driver**
   - Special routine responsible for loading required overlays into memory.
   - Handles transitions between overlays.
   - Ensures correct modules are in memory at the right time.

----------------------------------------------------
ðŸ“˜ WORKING:
1ï¸âƒ£ Program divided into modules.
2ï¸âƒ£ Overlay structure is created specifying which modules share memory.
3ï¸âƒ£ Overlay driver loads the necessary module when control transfers.
4ï¸âƒ£ Symbol table ensures correct addressing of shared/common routines.
5ï¸âƒ£ When module finishes, driver replaces it with the next required module.

----------------------------------------------------
ðŸ“— ADVANTAGES:
âœ… Efficient use of limited memory.
âœ… Enables large programs to run in small memory.
âœ… Reduces overall memory footprint.

----------------------------------------------------
ðŸ“™ DISADVANTAGES:
âŒ Complex programming and management.
âŒ Slower due to frequent I/O (loading overlays).
âŒ Manual design required (modern OS handle this automatically via virtual memory).

----------------------------------------------------------------------------------

ðŸ§  VIRTUAL MEMORY & PAGE FAULT â€” 

----------------------------------------------------
ðŸ“˜ VIRTUAL MEMORY (VM):

ðŸ”¹ Definition:
Virtual Memory is a memory management technique
that provides an illusion of a large, continuous main memory
to each process â€” even if physical RAM is smaller.

It allows processes to use more memory than physically available
by storing part of the data on disk.

----------------------------------------------------
ðŸ“— Key Concepts:
1ï¸âƒ£ Virtual Address Space (VAS)
   - Logical memory space assigned to each process.
   - Divided into pages (fixed size blocks).

2ï¸âƒ£ Physical Memory
   - Actual RAM divided into frames (same size as pages).

3ï¸âƒ£ Page Table
   - Maps virtual pages â†’ physical frames.
   - Used during address translation.

4ï¸âƒ£ Backing Store
   - Secondary storage (disk) used to hold pages not in RAM.
-----------------------------------------------------------
ðŸ“˜ trap
A trap in virtual memory is a CPU interrupt that occurs when a page fault happens â€” meaning the required page is not in main memory, so the OS must load it from disk.
----------------------------------------------------
ðŸ“˜ Working of Virtual Memory:
1. Process generates a Virtual Address (VA).
2. OS checks Page Table for mapping:
   - If present â†’ Physical Frame found â†’ Access directly.
   - If not present â†’ PAGE FAULT occurs.
3. If page fault occurs:
   - OS loads the missing page from disk into RAM.
   - Updates Page Table with the new mapping.
   - Then restarts the instruction.

----------------------------------------------------
ðŸ“— PAGE Hit:
ðŸ”¹ Definition
A Page Hit occurs when a process tries to access: 
a page that is already loaded into main memory (RAM).
----------------------------------------------------
ðŸ“— PAGE FAULT:

ðŸ”¹ Definition:
A Page Fault occurs when a process tries to access
a page that is not currently loaded into main memory (RAM).

----------------------------------------------------
ðŸ“˜ Steps in Handling a Page Fault:
1ï¸âƒ£ CPU generates a virtual address.
2ï¸âƒ£ OS checks page table â†’ "Valid bit = 0" â†’ Page not in RAM.
3ï¸âƒ£ OS suspends the process (page fault interrupt).
4ï¸âƒ£ Finds a free frame (or selects a victim page for replacement).
5ï¸âƒ£ Loads the required page from disk into the frame.
6ï¸âƒ£ Updates the Page Table (Valid bit = 1, Frame No = X).
7ï¸âƒ£ Restarts the interrupted instruction.

----------------------------------------------------
ðŸ“™ PAGE REPLACEMENT POLICIES:
When no free frame is available:
1ï¸âƒ£ FIFO (First In First Out)
2ï¸âƒ£ LRU (Least Recently Used)
3ï¸âƒ£ Optimal (Replace the page not needed for longest time)
4ï¸âƒ£ Clock / Second Chance Algorithm

----------------------------------------------------
ðŸ“˜ ADVANTAGES of Virtual Memory:
âœ… Allows large programs to run on small physical memory
âœ… Provides process isolation (security)
âœ… Efficient use of physical memory
âœ… Simplifies memory management

----------------------------------------------------
ðŸ“™ DISADVANTAGES:
âŒ Slower access due to disk I/O
âŒ More complexity in OS
âŒ Page Faults reduce performance (Thrashing risk)

ðŸ§  THRASHING â€” Operating System Notes

----------------------------------------------------
ðŸ“˜ DEFINITION:
Thrashing occurs when the operating system spends most of its time
swapping pages in and out of main memory instead of executing processes.

It happens when **the degree of multiprogramming is too high** and
there is **insufficient physical memory** to handle all active processes.

----------------------------------------------------
ðŸ“— KEY CONCEPT:
- Each process has a *working set* (pages it actively uses).
- If the total working sets of all processes > total available frames,
  the system will continuously swap pages (thrashing).

----------------------------------------------------
ðŸ“˜ CAUSES OF THRASHING:
1ï¸âƒ£ High degree of multiprogramming (too many processes in memory).  
2ï¸âƒ£ Insufficient number of frames.  
3ï¸âƒ£ Poor page replacement algorithm.  
4ï¸âƒ£ Sudden increase in process working set size.  
5ï¸âƒ£ Overcommitment of physical memory.
----------------------------------------------------
ðŸ“— EFFECTS OF THRASHING:
âŒ High page fault frequency  
âŒ Reduced throughput  
âŒ Low CPU utilization  
âŒ Increased I/O wait time  
âŒ System performance degradation  
----------------------------------------------------
ðŸ“˜ INDICATORS OF THRASHING:
1ï¸âƒ£ Very high page fault rate.  
2ï¸âƒ£ Low CPU utilization (CPU waiting for I/O).  
3ï¸âƒ£ Excessive disk I/O (swapping activity).  
4ï¸âƒ£ Overall system slowdown.

----------------------------------------------------
ðŸ“˜ PREVENTION / CONTROL TECHNIQUES:
1ï¸âƒ£ **Working Set Model** â€” Keep only active pages in memory.  
2ï¸âƒ£ **Page Fault Frequency (PFF)** â€” Control degree of multiprogramming dynamically.  
3ï¸âƒ£ **Use Better Replacement Algorithm (e.g., LRU).**  
4ï¸âƒ£ **Suspend or swap out processes** when page fault rate is too high.  
5ï¸âƒ£ **Increase Physical Memory (RAM)** if possible.  
----------------------------------------------------


21. Translation Lookaside Buffer (TLB):
        used for frequently refered page
        TLB (Translation Lookaside Buffer) is a small, fast cache in the memory management unit (MMU) that stores a subset of the page table entries to speed up the translation from logical to physical addresses. It holds the most frequently referenced pages, reducing the number of times the page table has to be consulted.
        TLB structure: 1.Tag - consist of page no. and process id   2.Key - consist of frame no related to page no in main memory

        TLB hit - A TLB hit occurs when the page number is found in the TLB, allowing direct access to the corresponding frame in memory.
        TLB miss - A TLB miss occurs when the page number is not in the TLB, requiring a lookup in the page table.

                    +------------------+
                    |    CPU           | 
                    | Logical Address  |
                    +------------------+
                             |
                             v
                  +---------------------------+
                  |  Translation Lookaside    |
                  |         Buffer (TLB)      |
                  +---------------------------+
                           |         |         
            +---------+    |    +----v----+      +-----------------+
            |   Hit   |<---+    |   Miss  |----> |Page Table Lookup|
            +---------+         +---------+      +-----------------+
               |                                    |
               |                                    v
               v                                +------------+
          Physical Frame                        |   Main     |
          (Accessed directly)                   |  Memory    |
                |                               +------------+
                |                                   ^
                |                                   |
                +-----------------------------------+

22. Demand Paging and Page Fault:
        Demand paging is a memory management scheme where pages are loaded into main/physical memory only when they are needed (i.e., when a page fault occurs). Initially, pages of a process are not loaded into memory, and they are brought in from disk (usually from the swap space) only when the
        A page fault occurs when a process tries to access a page that is not currently in main memory. The operating system must then handle the fault by loading the missing page into memory from disk.
        When the CPU sends a request for a particular page and a page fault occurs, a context switch happens from the user process to the OS process. The OS process loads the page from secondary memory to main memory, and then a context switch happens from the OS process back to the user process. When the same page request is made again by the CPU, the page is now present in main memory.

23. Virtual Memory:
        "Only the required or main pages of processes are loaded into main memory for efficient use of memory, and all pages of processes are represented by P/A in page table entries with the help of the P/A bit

        Process P1 Pages                Page table entries for P1               main memory

        +---------+                     +-------------+------+                  +----------------+
        |   0     |                     |Frame number | P/A  |                  |   P1(0)        |
        |   1     |                     +-------------+------+                  |   P1(3)        |
        |   2     |                   0 |   F1        |  p   |                  |   P1(6)        |
        |   3     |                   1 |   -         |  A   |                  |   P1(9)        |
        |   4     |                   2 |   -         |  A   |                  |   P1(10)       |
        |   5     |                   3 |   F2        |  P   |                  |   P2(2)        |
        |   6     |                   4 |   -         |  A   |                  |   P2(7)        |
        |   7     |                   5 |   -         |  A   |                  |   P5(4)        |
        |   8     |                   6 |   F3        |  P   |                  |   P5(7)        |
        |   9     |                   7 |   -         |  A   |                  |   P5(9)        |
        |  10     |                   8 |   -         |  A   |                  +----------------+
        +---------+                   9 |   F4        |  P   |
                                     10 |   F5        |  P   |
                                        +-------------+------+
    
    1. Frame Allocation:
            The process in which pages of a process are assigned frames in main memory, ranging from a minimum number of frames (the number of pages required to execute one instruction of a process) to a maximum number of frames (the total number of pages of the entire process).

            Frames allocation techniques:
                1. Equal Allocation: 
                    Allocates an equal number of frames to each process. e.g. P1 = 20 pages, P2 = 500 pages, P3 = 200 pages, main memory frames = 60, then 60 / 3 = 20 frames each for every process.
                2. Weighted Allocation: 
                    Allocates frames based on process with how many number of pages it has (weight). e.g. P1 = 30 pages, P2 = 30 pages, P3 = 40 pages, main memory frames = 10, total pages = (P1)30 + (P2)30 + (P3)40 = 100, then P1 gets (30 / 100) * 10 = 3 frames, P2 gets (30 / 100) * 10 = 3 frames, P3 gets (40 / 100) * 10 = 4 frames
                3. Dynamic Allocation: 
                    Allocates frames based on process priority (replaces pages of process with leaset priority with pages of process with high priority)

    2. Page Replacement:
            The process of replacing existing pages of a particular process to load the required pages for execution, ensuring optimal use of virtual memory.

            Page Replacement Algorithms:
                1. FIFO (First-In, First-Out)
                    FIFO replaces the oldest page in memory. The page that was loaded first will be replaced when a new page needs to be loaded into memory.

                    Reference String: 4, 7, 6, 1, 7, 6, 1, 2, 7, 2
                    Frames in memory: [ - , - , - ]  (Initial state)

                    Step 1: Reference 4 â†’ Load 4 into frame 1 â†’ [ 4, - , - ]
                    Step 2: Reference 7 â†’ Load 7 into frame 2 â†’ [ 4, 7, - ]
                    Step 3: Reference 6 â†’ Load 6 into frame 3 â†’ [ 4, 7, 6 ]
                    Step 4: Reference 1 â†’ Replace 4 (oldest) with 1 â†’ [ 1, 7, 6 ]
                    Step 5: Reference 7 â†’ 7 is already in memory â†’ [ 1, 7, 6 ]
                    Step 6: Reference 6 â†’ 6 is already in memory â†’ [ 1, 7, 6 ]
                    Step 7: Reference 1 â†’ 1 is already in memory â†’ [ 1, 7, 6 ]
                    Step 8: Reference 2 â†’ Replace 7 (oldest) with 2 â†’ [ 1, 2, 6 ]
                    Step 9: Reference 7 â†’ Replace 6 (oldest) with 7 â†’ [ 1, 2, 7 ]
                    Step 10: Reference 2 â†’ 2 is already in memory â†’ [ 1, 2, 7 ]

                    queue = [4, 7, 6, 1, 2, 7]

                    **Page Faults**: 6 page faults  
                    **Final Frame Content**: [ 1, 2, 7 ]

                    # Belady's Anomaly in LRU (OS)

                    Beladyâ€™s Anomaly â†’ When increasing the number of page frames causes *more* page faults.  
                    # FIFO may give more faults with 4 frames than with 3.


                2. LRU (Least Recently Used)
                    In LRU, we replace the page that has not been used for the longest time. The page replacement decision is based on the order of recent accesses.

                    Reference String: 4, 7, 6, 1, 7, 6, 1, 2, 7, 2
                    Frames in memory: [ - , - , - ]  (Initial state)

                    Step 1: Reference 4 â†’ Load 4 into frame 1 â†’ [ 4, - , - ]
                    Step 2: Reference 7 â†’ Load 7 into frame 2 â†’ [ 4, 7, - ]
                    Step 3: Reference 6 â†’ Load 6 into frame 3 â†’ [ 4, 7, 6 ]
                    Step 4: Reference 1 â†’ Replace 4 (least recently used) with 1 â†’ [ 1, 7, 6 ]
                    Step 5: Reference 7 â†’ 7 is already in memory â†’ [ 1, 7, 6 ]
                    Step 6: Reference 6 â†’ 6 is already in memory â†’ [ 1, 7, 6 ]
                    Step 7: Reference 1 â†’ 1 is already in memory â†’ [ 1, 7, 6 ]
                    Step 8: Reference 2 â†’ Replace 7 (least recently used) with 2 â†’ [ 1, 2, 6 ]
                    Step 9: Reference 7 â†’ Replace 6 (least recently used) with 7 â†’ [ 1, 2, 7 ]
                    Step 10: Reference 2 â†’ 2 is already in memory â†’ [ 1, 2, 7 ]

                    **Page Faults**: 6  
                    **Final Frame Content**: [ 1, 2, 7 ]
                
                3. Optimal Page Replacement
                    In the Optimal algorithm, the page that will not be used for the longest period in the future is replaced. This is the ideal strategy, but in practice, it's impossible to implement because it requires knowledge of future references.

                    Reference String: 4, 7, 6, 1, 7, 6, 1, 2, 7, 2
                    Frames in memory: [ - , - , - ]  (Initial state)

                    Step 1: Reference 4 â†’ Load 4 into frame 1 â†’ [ 4, - , - ]
                    Step 2: Reference 7 â†’ Load 7 into frame 2 â†’ [ 4, 7, - ]
                    Step 3: Reference 6 â†’ Load 6 into frame 3 â†’ [ 4, 7, 6 ]
                    Step 4: Reference 1 â†’ Replace 4 (will not be used for the longest time) with 1 â†’ [ 1, 7, 6 ]
                    Step 5: Reference 7 â†’ 7 is already in memory â†’ [ 1, 7, 6 ]
                    Step 6: Reference 6 â†’ 6 is already in memory â†’ [ 1, 7, 6 ]
                    Step 7: Reference 1 â†’ 1 is already in memory â†’ [ 1, 7, 6 ]
                    Step 8: Reference 2 â†’ Replace 1 (will not be used for the longest time) with 2 â†’ [ 2, 7, 6 ]
                    Step 9: Reference 7 â†’ 7 is already in memory â†’ â†’ [ 2, 7, 6 ]
                    Step 10: Reference 2 â†’ 2 is already in memory â†’ [ 2, 7, 6 ]

                    **Page Faults**: 5  
                    **Final Frame Content**: [ 2, 7, 6 ]
                
                4. Most Recently Used (MRU)
                    In the MRU algorithm, the page that was used most recently is replaced.
                    The assumption is that the most recently used page is less likely to be needed soon.
                    This is opposite of the LRU strategy.

                    Reference String: 4, 7, 6, 1, 7, 6, 1, 2, 7, 2
                    Frames in memory: [ - , - , - ]  (Initial state)

                    Step 1: Reference 4 â†’ Load 4 â†’ [ 4, - , - ]
                    Step 2: Reference 7 â†’ Load 7 â†’ [ 4, 7, - ]
                    Step 3: Reference 6 â†’ Load 6 â†’ [ 4, 7, 6 ] (All frames are full now)
                    Step 4: Reference 1 â†’ Replace MRU = 6 â†’ [ 4, 7, 1 ] (Most recently used page before this step was 6)
                    Step 5: Reference 7 â†’ Already in memory â†’ [ 4, 7, 1 ] (MRU becomes 7)
                    Step 6: Reference 6 â†’ Not in memory â†’ Replace MRU = 7 â†’ [ 4, 6, 1 ]
                    Step 7: Reference 1 â†’ Already in memory â†’ [ 4, 6, 1 ] (MRU becomes 1)
                    Step 8: Reference 2 â†’ Replace MRU = 1 â†’ [ 4, 6, 2 ]
                    Step 9: Reference 7 â†’ Not in memory â†’ Replace MRU = 2 â†’ [ 4, 6, 7 ]
                    Step 10: Reference 2 â†’ Not in memory â†’ Replace MRU = 7 â†’ [ 4, 6, 2 ]

                    **Page Faults**: 7  
                    **Final Frame Content**: [ 4, 6, 2 ]


24. Segementation (same like pagination, close to user understanding):
        Logical Address: The address generated by the CPU during execution, consisting of a segment number and an offset.
        Physical Address: address in main memory with base and offset less than limit
        Segment Table (SGT): A data structure that maps logical segment numbers to their corresponding physical addresses in memory.
        Segment Table Base Register (SGTB): Holds the starting address of the segment table in physical memory.
        Base: The starting address of a segment in physical memory.
        Limit: The size of the segment, which defines the valid range of the offset.
        Offset: A value indicating a specific location within a segment.

                       (base, offset)
           +---------+        |     +-----------------+           +-----------------+
           |  CPU    |        |      | Seg. Table Base |           |  Main Memory    |
           | Logical | --(LA)------> | Register (STBR) |           +-----------------+
           | Address |               +-----------------+           |     OS          |
           +---------+                              |     base +---+-----------------+
                                                    |    limit |                     |
          +----------------------------------+      |          +---------------------+
          |             Seg. Table           |      |          |                     | <------+
          +----------------------------------+      |          +---------------------+        |
          |  Base      |  Limit              |      |          |                     |        |
          +------------+---------------------+ <----+          +---------------------+        |
        0 |     0      |      3              |                 |                     |        |
        1 |     1      |      4              |                 +---------------------+        |                       
        2 |     2      |      2              | ------------(PA)-------------------------------+                                      
          +----------------------------------+

25. Priority Inheritance:
        If a low-priority process enters the critical section and, at the same time, a high-priority process arrives and also wants to enter the critical section, mutual exclusion ensures that only one process can be in the critical section at a time. To prevent the low-priority process from blocking the high-priority process (leading to a potential priority inversion), the low-priority process inherits the priority of the high-priority process.
        As a result, the low-priority process continues to execute in the critical section until it completes. Once the low-priority process exits the critical section, it reverts back to its original low priority, and then the high-priority process can enter the critical section.
        This mechanism of priority inheritance is a solution to priority inversion, where a higher-priority process is indirectly blocked by a lower-priority one, potentially causing delays in the system.
        If preemption occurs on P1, such that the CPU is allocated to the high-priority process (P2) while the low-priority process (P1) is still in the critical section, a deadlock or spinlock situation can arise. In this case, P1 has already entered the critical section but needs the CPU to continue execution, while P2 has the CPU but cannot enter the critical section due to mutual exclusion constraints. This leads to a deadlock or spinlock, where neither process can proceedâ€”P1 is blocked from continuing, and P2 is blocked from entering the critical section.
        Priority inheritance helps avoid this scenario by ensuring that P1 temporarily inherits the priority of P2, allowing P1 to complete its critical section and release the CPU, thereby preventing deadlock or spinlock.

                    P2 (high priority)

        +----------+
        | entry    |
        +----------+

        CS (P1 with low priority)
                (P1 inherits P2 priority and becomes high priority process)

        +----------+
        | exit     |
        +----------+
                (P1 again come back to previous low priority)

26. Inter Process Communication (IPC):
        Independent Processes: These processes do not need to communicate with other processes. They run independently and do not share data.
        Cooperative Processes: These processes need to communicate or synchronize with each other to accomplish a task. They share data or coordinate their actions.

        Inter-Process Communication (IPC)
        IPC allows processes to exchange data or synchronize their actions. The two main models of IPC are:

        1. Shared Memory:
            two process share a memory one process write data in that shared memory and one will read that data to communicate. 

            +------+
            | P1   |--write--+
            +------+         |
            |shared| <-------+
            |memory| --------+
            +------+         |
            | P2   | <--read-+
            +------+
        2. Message passing:
            one process send message to kernal and kernal send that message to perticular recever process and that process receives that message which is how communication happens

            +------+
            | P1   |--send------+
            +------+            |
            |P2    |<-receive-+ |
            +------+          | |
            |      | ---------+ |
            |Kernal| <----------+
            +------+

27. Semaphores:
        A semaphore is an integer variable used to manage access to shared resources in concurrent processes. It was introduced by Edsger Dijkstra in 1965 as a synchronization tool to handle mutual exclusion and synchronization between processes.

        Semaphores are used to control access to critical sections (shared resources) in a way that prevents race conditions and ensures that processes run in an orderly fashion.

        Types of Semaphores (semaphore may be application level or kernal/os level)
            1. Binary Semaphore (also called a Mutex Semaphore)
                A binary semaphore can only take two values: 0 or 1. It is used to ensure mutual exclusion in critical sections, where only one process can execute in the critical section at a time.

                Operations:
                P(s) / wait / down: This operation checks if the semaphore value is 0 (indicating that the resource is unavailable). If it is, the process is blocked (suspended) until the value becomes 1. If the value is 1, it is decremented to 0 (indicating the resource is now locked).
                V(s) / signal / up: This operation increments the semaphore value, releasing the resource. If any process was blocked on the semaphore, it is now allowed to proceed.

                P(semaphore s) {
                    while (s == 0);   // If semaphore is 0, block the process
                    s = s - 1;        // Otherwise, decrement semaphore to 0 (enter critical section)
                }

                V(semaphore s) {
                    s = s + 1;        // Increment semaphore to 1 (exit critical section, allow another process)
                }

            P1:
                P(s);           // Enter critical section (decrement semaphore to 0)
                // Critical section code here
                V(s);           // Exit critical section (increment semaphore to 1)

            P2:
                P(s);           // Wait for P(s) to be 1 and enter critical section
                // Critical section code here
                V(s);           // Exit critical section



            2. Counting Semaphore
               A counting semaphore is used when the resource being managed has multiple instances, and the semaphore maintains a count of available resources. The semaphore value can range from 0 to any positive integer. The processes are blocked if the semaphore count is 0 (i.e., no resources are available).

                Operations:

                P(s) / wait / down: If the semaphore value is greater than 0, it is decremented. If the semaphore value is 0, the process is added to the block list (i.e., the process is suspended until resources are available).
                V(s) / signal / up: The semaphore value is incremented. If there are any processes waiting in the block list, one of them is moved to the ready queue, allowing it to continue execution.

                P(semaphore s) {
                    while (s == 0);   // If semaphore is 0, block the process
                    s = s - 1;        // Otherwise, decrement semaphore (access resource)
                }

                V(semaphore s) {
                    s = s + 1;        // Increment semaphore (release resource)
                    // If any processes are waiting, wake one up
                }


                Semaphore s = 3;  // 3 printers available

            P1:
                P(s);         // Decrement semaphore (use a printer)
                // Use printer (critical section code)
                V(s);         // Increment semaphore (release printer)

            P2:
                P(s);         // Decrement semaphore (use another printer)
                // Use printer (critical section code)
                V(s);         // Increment semaphore (release printer)

            P3:
                P(s);         // Decrement semaphore (use another printer)
                // Use printer (critical section code)
                V(s);         // Increment semaphore (release printer)

28. Message Queues

        Message queues in an OS are stored in the kernel in the form of a linked list.
        Message queues do not strictly follow FIFO (First In, First Out) order.
        A message consists of data and a type, where the type is represented as an integer value.
        Message queues are used for inter-process communication (IPC).
        System calls related to message queues:
            `msgget()`: Creates a new queue or opens an existing queue.
            `msgsnd()`: Writes data to the message queue.
            `msgrcv()`: Reads data from the message queue.
            `msgctl()`: Performs control operations on the message queue.

29. Pipes:
        Pipe communication between processes:

        P0 (write) â†’ pipe â†’ P1 (read)
        Used for communication between parent and child processes.
        Follows FIFO (First In, First Out) order.
        You can perform a write of 512 bytes at a time in a pipe and read 1 byte at a time from the pipe.
        
        System call to create a pipe:
        int pipe(int fds[2]); //This creates two file descriptors: fds[0] (read) and fds[1] (write).
        Return 0 on success (pipe successfully created).
        Return -1 on failure (use perror() to determine the cause of failure).

        System calls:
        write(fd[1], ...) to write into the pipe.
            Returns the number of bytes written or -1 on failure (use perror() to determine the reason for failure).
        read(fd[0], ...) to read from the pipe.
            Returns the number of bytes read or -1 on failure (use perror() to determine the reason for failure).

        To establish two-way communication between parent and child, you can use two pipes:
                              +----+
                       +----->|pipe|------+
            +------+   |      +----+      |    +-----+
            |parent| --+                  +--> |child|
            +------+ <--+                 +--  +-----+
                        |      +----+     |
                        +------|read|<----+
                               +----+

30. interrupts:
        An interrupt is a mechanism that temporarily halts the execution of the current process to give attention to a higher-priority task, and then resumes the original process after the task is handled.

        Types of interrupts:
            1. Internal Interrupt: Triggered by events within the CPU, such as a division by zero or invalid memory access.
            2. External Interrupt: Triggered by external devices, such as hardware interrupts from peripherals (e.g., keyboard, mouse, or timer).
            3. Software Interrupt: Triggered by software instructions, like system calls or exceptions. Example: INT instruction in x86 assembly.

        At the atomic level, system execution refers to the execution of individual instructions.
        Interrupts are checked after completing the execution of one interrupt. Interrupts cannot be checked while an interrupt is being executed.

        Interrupt Flag (IF):
            The Interrupt Flag (IF) is used to check whether interrupts are enabled or not.
        
        Interrupt Service Routine (ISR):
            Before entering the ISR, the CPU saves the current status (CPU registers or the current process's PCB) to preserve the execution context.
            The last instruction of the ISR is typically a return from interrupt (e.g., IRET or RTI instruction), which restores the CPU's state and resumes execution of the interrupted process.

        Interrupt Vector Table (IVT):
            The first instruction of the ISR is located by referring to the Interrupt Vector Table (IVT), which contains memory addresses of ISRs for different interrupts. The IVT is indexed by the interrupt number.
        
        After the ISR completes, the system loads the next instruction of the interrupted program and continues the execution of the process.

31. Enabling and Disabling Interrupts:
        Interrupts can be enabled or disabled by setting or clearing specific bits in the Interrupt Enable (IE) register (by setting 1/0)

        IE Register (Interrupt Enable Register): 8 bit register which controls interrupts are enabled and disabled in the system.

        EA (Bit 7): Global Interrupt (1 = enable all interrupts, 0 = disable all interrupts)
        EX1 (Bit 0): External Interrupt 1 
        EX0 (Bit 2): External Interrupt 0 
        ET1 (Bit 1): Timer 1 Overflow Interrupt 
        ET0 (Bit 3): Timer 0 Overflow Interrupt 
        ES (Bit 4): Serial Port Interrupt 
        Reserved (Bits 5, 6): Reserved for future use

        +-----+-----+-----+-----+-----+-----+-----+-----+
        | EA  |  -  |  -  | ES  | ET1 | EX1 | ET0 | EX0 |
        +-----+-----+-----+-----+-----+-----+-----+-----+
           7     6     5     4     3     2     1     0

Hard Disk Architecture (OS)

A Hard Disk Drive (HDD) consists of multiple platters that store data magnetically.
Each platter has two surfaces, and each surface is accessed by a read/write head.
The actuator arm moves the heads across tracks, and the spindle rotates the platters.

Components:
- Disk / Platters
- Surfaces
- Tracks
- Sectors
- Spindle
- Actuator Arm
- Read/Write Heads
- Data
                         actuator Arm
           spindle         |
              |     +------|
  disk  -------------      |
              |     +------|  
              |            |
              |     +------|
        -------------      |
              |     +------|
              |            |
              |     +------|
        -------------      |
              |     +------|
              |            |
              |     +------|
        -------------      |
              |     +------|
              |            |
              |     +------|
        -------------      |
                    +------|
                     |
            read\write heads

        +-------------------------------+
        |            TRACK              |
        |   (Concentric circular path)  |
        |     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   |
        |     |          +------+   |   |
        |     â”‚SECTOR    | data |   |â—„â”€â”€ Sector (slice of track)
        |     |          +------+   |   |
        |     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   |
        +-------------------------------+


DATA ORGANIZATION (Logical)

Platter â†’ Surface â†’ Track â†’ Sector â†’ Data Block

Example:
    Track 12, Sector 6  â†’  Contains 512 bytes / 4 KB (typical)

Seek Time (Ts): Time for head to move to target track.
Formula: Ts = (number of tracks moved) Ã— (seek time per track)

Rotation Time (Tr): Time for one full rotation.
Formula: Tr = 1 / Rotation Speed (RPS)

Rotational Latency (Tl): Average time waiting for sector.
Formula: Tl = 0.5 Ã— Rotation Time

Transfer Time (Tt): Time to read/write data.
Formula: Tt = Data Size / Transfer Rate

Transfer Rate = Number of Heads Ã— Capacity of One Track Ã— Rotations per Second (RPS)
Formula: R = Data Size / Transfer Time

Controller Time (Tc): Time for controller to process request.
Formula: Tc = constant (device-dependent)

Disk Access Time (Ta): Total time to access data.
Formula: Ta = Ts + Tl + Tt + Tc

Disk Scheduling Algorithms
1) FCFS (First-Come-First-Serve)
   Sequence: 50 â†’ 82 â†’ 170 â†’ 43 â†’ 140 â†’ 24 â†’ 16 â†’ 190
   Total seek distance = 642 tracks
   Seek time = 642 Ã— (seek_time_per_track)

2) SSTF (Shortest Seek Time First)
   Sequence: 50 â†’ 43 â†’ 24 â†’ 16 â†’ 82 â†’ 140 â†’ 170 â†’ 190
   Total seek distance = 208 tracks
   Seek time = 208 Ã— (seek_time_per_track)

3) SCAN (Elevator) â€” moving RIGHT initially, goes to end (199) then reverses
   Sequence: 50 â†’ 82 â†’ 140 â†’ 170 â†’ 190 â†’ 199 â†’ 43 â†’ 24 â†’ 16
   Total seek distance = 332 tracks
   Seek time = 332 Ã— (seek_time_per_track)

4) C-SCAN (Circular SCAN) â€” moving RIGHT, jump from end to start NOT counted
   Sequence (counted movements): 50 â†’ 82 â†’ 140 â†’ 170 â†’ 190 â†’ 199  (jump) 0 â†’ 16 â†’ 24 â†’ 43
   Total seek distance = 192 tracks   (jump 199â†’0 not counted)
   Seek time = 192 Ã— (seek_time_per_track)

5) C-LOOK (Circular LOOK) â€” move right to highest request, jump to lowest request (jump not counted)
   Sequence: 50 â†’ 82 â†’ 140 â†’ 170 â†’ 190  (jump) 16 â†’ 24 â†’ 43
   Total seek distance = 167 tracks   (jump not counted)
   Seek time = 167 Ã— (seek_time_per_track)

6) LOOK
   - Head moves in one direction and serves requests until the last request in that direction,
     then reverses (does NOT go to the physical disk end).

   Assumed initial direction: RIGHT

   Sequence:
     50 â†’ 82 â†’ 140 â†’ 170 â†’ 190 â†’ 43 â†’ 24 â†’ 16

   Total seek distance = 314 tracks
   Seek time = 314 Ã— (seek_time_per_track)

------------------------------------------------------------
Best (minimum total seek) : C-LOOK  â†’ 167 tracks
Worst (maximum total seek): FCFS   â†’ 642 tracks
-------------------------------------------------------------

====================================================
ðŸŸ¦ HOW FILES ARE STORED ON DISK (Short + Simple)
====================================================

A file is divided into **fixed-size blocks** (like pages in RAM).
Disk is divided into **sectors â†’ blocks** (e.g., 512B, 4KB).

FILE â†’ split into BLOCKS:
   File F = [ B0 | B1 | B2 | B3 | ... ]

These blocks must be stored somewhere on disk.
Different OS use different methods:

====================================================
1) CONTIGUOUS ALLOCATION
====================================================

File occupies consecutive disk blocks (like an array).

 Diagram:
   Disk Blocks â†’
   [100][101][102][103][104][105][106]...
          ^---------- File F --------^

 Directory Entry:
   F â†’ (start = 101, length = 4)

 Notes:
  âœ” Fast sequential & random access  
  âœ˜ External fragmentation  
  âœ˜ File cannot grow easily  

====================================================
2) LINKED (CHAINED) ALLOCATION
====================================================

Each file block points to the next block (like a linked list).

 Diagram:
   F â†’ [120] â†’ [305] â†’ [412] â†’ [500] â†’ NULL

 Block structure:
   [ data | next_block ]

 Directory Entry:
   F â†’ (first_block = 120)

 Notes:
  âœ” Easy to grow file  
  âœ” No external fragmentation  
  âœ˜ Slow random access (must follow chain)  
  âœ˜ Pointer loss breaks file  

====================================================
3) INDEXED ALLOCATION
====================================================

One **index block** stores all pointers to the fileâ€™s data blocks.
(Like a PAGE TABLE for a file!)

 Diagram:
      index block (700)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚120 â”‚ 305 â”‚ 412 â”‚ 560 â”‚ ... â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“     â†“     â†“     â†“
        [120] [305] [412] [560] ...

 Directory Entry:
   F â†’ (index_block = 700)

 Notes:
  âœ” Fast random access (direct jump)  
  âœ” Easy to grow  
  âœ˜ Index block overhead  
  âœ˜ Very large files need multi-level index  

====================================================
4) UNIX INODE (Indexed + Levels)
====================================================

Inode stores file metadata + pointers:
 - 12 direct pointers
 - 1 single indirect (one-level index)
 - 1 double indirect (two levels)
 - 1 triple indirect (three levels)

 Diagram:
    inode(42)
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ direct[0..11] â”‚ single â”‚ double â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“             â†“        â†“
        data         index     index-of-index

 Notes:
  âœ” Efficient for small & medium files  
  âœ” Scales to huge files  
  âœ” Used in UNIX/Linux (ext3/ext4)  
  âœ˜ Indirection overhead for very large offsets  
  ----------------------------------------------------


1 bit  = 2^0  bits
1 Byte (B) = 8 bits = 2^3 bits
1 KB = 2^10 B
1 MB = 2^20 B
1 GB = 2^30 B
1 TB = 2^40 B
1 PB = 2^50 B
1 EB = 2^60 B

2^1  = 2
2^2  = 4
2^3  = 8
2^4  = 16
2^5  = 32
2^6  = 64
2^7  = 128
2^8  = 256
2^9  = 512
2^10 = 1024
